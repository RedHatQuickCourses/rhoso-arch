:time_estimate: 18

= OpenStack Services as OpenShift Workloads

_Estimated reading time: *{time_estimate} minutes*._

Goal::

Describe how OpenStack services run as Kubernetes workloads and pods.

== OpenStack Services as Containers

Packaging and running OpenStack services as containers is not a new idea from Red Hat OpenStack Services on OpenShift. The OpenStack community was already experimenting with different ways of doing that, with and without Kubernetes. In fact, previous incarnations of the Red Hat OpenStack Platform run OpenStack services as podman containers and manage these containers using Linux systemd and Ansible.

Despite all the advantages of just packaging and running individual applications as containers, complex service-oriented applications benefit from an orchestration layer and Kubernetes is established as the preferred container orchestration tool. Red Hat OpenShift, as an application platform based on Kubernetes, benefits an OpenStack control plane as a whole and also individual services of OpenStack with its many components, such as Nova.

OpenStack itself has a multi-layered architecture: from its user-facing and infrastructure services, passing through the individual components of each service, down to the operating system processes of each component. Red Hat OpenStack Services on OpenShift maps this multi-layered architecture to Kubernetes add-on operators, Kubernetes workloads, Kubernetes pods, and containers.

As a Linux System Administrator, an OpenStack Administrator is expected to have basic knowledge of https://www.redhat.com/en/topics/containers/whats-a-linux-container[Linux Containers], or simply containers, and how they differ from Virtual Machines (VMs). If you are new to containers, we suggest that you pause to review the relevant sections of https://www.redhat.com/en/services/training/rh134-red-hat-system-administration-ii[RH134] and https://www.redhat.com/en/services/training/do080-deploying-containerized-applications-technical-overview[DO080].

This section continues the explanation of Kubernetes concepts related to managing containerized applications with a top-down approach: the previous sections presented Kubernetes add-on operators, and this section presents Kubernetes workloads, pods, and containers.

== Kubernetes Application Resources

Kubernetes represents containerized applications using many API resource types, and the main one is the *Pod*, which represents a group of containers sharing Linux kernel namespaces. Pods are managed by a workload controller, such as a Deployment or a Job. Pods need configuration data, which is represented by Secrets and Configuration Maps. Connectivity between Pods, or from outside of a Kubernetes cluster and its Pods, comes from Services and Ingress or from OpenShift Routes. Finally, persistent storage mounted by Pods is represented by Persistent Volume Claims (PVCs).

image::s3-workloads-lecture-fig-1.svg[]

Kubernetes Operator users can only create and manage resources inside the projects they have access to, but they also have read-only access to selected resources outside their projects, such as Storage Classes and Persistent Volumes.

The next chapter will focus on the API resources related to storage and networking, including OpenShift custom resources not shown in the figure. This section will focus on the API resources required to manage containers and their configuration data.

=== Kubernetes Namespaces and OpenShift Projects

Kubernetes organizes API resource instances in Namespaces, and users are granted access to projects using Role-Based Access Control (RBAC). Kubernetes Operators are granted the `edit` role in a project, meaning they can create and manage resource instances of a restricted set of resource types, which allows them to deploy and manage applications. Kubernetes Administrators get the `admin` role, which allows them to create and manage instances or a larger set of resource types, including Role Binding resources and Resource Quotas.

// With this busy figure, the arrow heads are not very visible as PNG. Need to fix the SVG issue for better quality of diagrams.

image::s3-workloads-lecture-fig-2.svg[]

Most Kubernetes API resource types are namespaced resources, meaning they must exist inside a Kubernetes Namespace, and their names would not collide with resources of the same type in different Namespaces. A few API resource types are non-namespaced, meaning they exist outside any Namespace. Kubernetes Cluster Administrators get the `cluster-admin` role to manage those non-namespaced resources, and also instances of all resource types in all namespaces.

A Kubernetes Operator may get `admin` in some namespaces but not in others, meaning they are namespace administrators instead of cluster administrators. An OpenStack Administrator may either be an administrator of the two projects of the OpenStack add-on operator or be just an Operator of those projects, getting the `edit` role.

With standard Kubernetes, only Cluster Administrators can create namespaces, which is too restrictive for application developers: they must either ask an administrator to create and configure projects for them, or pile too many unrelated applications in the namespaces they already have access to.

Red Hat OpenShift extends Kubernetes with Projects, which are, for most practical purposes, the same as Kubernetes namespaces. Any user of OpenShift, of a selected group of users granted the `self-service` role, can create new Projects and these become Kuberentes namespaces.

Those self-service namespaces are created with a default set of API resources, which is predefined by Red Hat to grant the project creator `admin` on their new projects, and can be further customized by cluster administrators to include more resource instances or give fewer privileges to the project creator.

=== Kubernetes Workloads

The concept of application varies with programming language, architecture style, and other factors, so Kubernetes offers a number of resource types to represent applications. These same resource types could represent only a piece of a larger application, such as a component of an OpenStack service. These resource types are part of the Kubernetes Workloads API.

image::s3-workloads-lecture-fig-3.svg[]

Kubernetes workload resource types, also known as Kubernetes workload controllers, represent the scalability and availability requirements of an application or component, for example: a network server application, such as an OpenStack API endpoint, must be running continuously. If, for any reason, that API endpoint stops running, a workload controller will start another instance, so users can always invoke those APIs.

Not all applications must run continuously, and sometimes you must run many copies of the same application, no matter in which node of a Kubernetes cluster. Other times, it does matter which node, and you may need exactly one instance per cluster node, but not more or less. The different types of Kubernetes workload controllers deal with those varying application requirements:

Replica Set::

Represents a stateless application that runs multiple Pods that are copies, or replicas, of each other, in the sense that any user request could be processed by any of the replicas and responding to user requests does not depend on any state stored on the memory of a specific Pod. Replica pods do not keep their identity, if recreated, they cannot restore any data specific to the pod from an external store such as a cache service. It is the workload controller that provides horizontal scalability for applications, but it is rarely used directly: Kubernetes Operators typically create, deployments which in turn create Replica Sets.

Deployment::

Represents a stateless application and manages its upgrade process, by either rolling out new Pods, while deleting old Pods, little by little, or by replacing all old pods at once with new pods. If the application design supports it, progressive rolling out new Pods is better than replacing all at one, because it avoids downtime as perceived by end users. Old and new pods are managed as two Replica Sets. Deployments are the most common workload controllers with cloud-native applications on Kubernetes.

Job:: 

Represents an application which runs once through completion. A Job can retry a number of times, and whatever the end result, a Job resource instance keeps the status of completion, either successful or not (after attempting all retries). You may see many jobs that fire at OpenStack add-on operator installation, or at significant events such as adding new compute nodes to an OpenStack cluster.

Cron Job::

Represents an application which runs with a recurrent schedule. Each individual execution of the schedule is a Job resource. Cron Jobs fit nicely periodic tasks, such as purging records marked for deletion from a database.

Stateful Set::

Represents an application which may require multiple Pods, similar to a Replica Set, but that are not replicas of each other. Each Pod in the set performs different tasks and may depend on the state stored in its own memory to continue executing those tasks. Pods from a Stateful Set must synchronize state between themselves and they retain their identity, so a replacement Pod can resume the work of a specific previous Pod. They must be designed to retrieve their state from other Pods of their set or from an external store, such as a database. Examples of workloads that fit a StatefulSet include primary/secondary databases, such as relational databases with one read-write and multiple read-only replicas, and NoSQL databases with multiple shards.

Daemon Set::

Represents an application which must run in all cluster node or a subset of those nodes. It ensures there is always one and only one Pod of that workload running in each and all of the eligible Kubernetes cluster nodes. Controllers for hardware accelerator cards are typical examples of Daemon Sets.

It's common with Kubernetes that multiple API resource types work in layers, or as different abstraction levels, to manage one application or a piece of a larger application. For example:

* A Deployment instance manages one or two Replica Set instances, which in turn manage one or more Pod instances each.

* A Cron Job instance creates new Job instances every time its schedule requires. Each Job instance manages one Pod instance, or possibly multiple Pod instances if there are retries.

That design pattern of resource instances which manage other resource instances is extended to Kubernetes add-on operators, where each add-on operator instance may manage multiple workload controllers, and a custom resource from an add-on operator may manage multiple application workloads.

=== Stateless and Stateful Componentes of OpenStack Control Planes

OpenStack services use a stateless API design but that does not mean all OpenStack services and their components are stateless. Many factors come in the implementation of individual components which may require internal state and synchronization between instances of that component or with other components of the same service.

Locality is an important factor that affects performance and reliability and few OpenStack services have a pure stateless design, despite all of them delegating the persistence of their API resources to relational databases.

For example, the API component of an OpenStack service may also manage data stored in an external storage backend, which is accessed by workloads running as VMs on compute nodes. Or that API component may use caching to improve its performance.

Many Kubernetes Operators, use the https://12factor.net/[12-factor] style of cloud-native application design, and would expect that OpenStack services run mostly as Kubernetes Deployments. They may be surprised to find that so many services run entirely from Stateful Sets. Most times, the determining factor for the Red Hat OpenStack Services on OpenShift engineering choice was the need to preserve the network identity of individual pods of a component, which you cannot do with Kubernetes Deployments.

== Kubernetes Pods and Containers

All isolation between containers comes from starting them inside different Linux Kernel namespaces. Unlike virtual machines, which are always completely isolated from their host operating system, a container may be only partially isolated, depending on its namespaces. Kubernetes offers control of which namespaces are isolated or not for groups of containers, called Pods, and OpenShift uses that to enable platform-level components which provide advanced networking, storage, and security features.

=== Containers and Linux Kernel Namespaces in Kubernetes

There are many types of Kernel namespaces: uid namespaces, mount namespaces, process namespaces, network namespaces, and so on. Containers do not require all types of namespaces, and containers may belong to different namespaces that intersect with other containers. Containers may run on host namespaces, which enables them to perform privileged operations on a node of an OpenStack or OpenShift cluster.

For example, a container may run on its isolated process and mount namespace, but not run on the network namespace of its host, and this way manage host networking. Like NMState and MetalLB add-on operators to.

Another example, is a container may run on its isolated process and network namespaces, but not on the mount namespaces, and thus control storage devices and mount points of its host. Like any CSI driver such as the LVMS add-on operator or OpenShift Data Foundation.

WARNING: Kubernetes currently does not take advantage of all types of Kernel namespaces available in Linux, and some of these namespaces are very tricky to use together, such as the uid or user namespace versus the mount namespace. The tricky part comes from managing files and group ownership of files, thus current releases of Kubernetes run all containers without user namespace isolation: a container running in Kubernetes as the Linux root user has all root user privileges on its host, unless constrained by other Linux features such as SELinux and Kernel capabilities.

=== Kubernetes Pods

A Pod in Kubernetes is a group of containers which share some Kernel namespaces, especially the network namespace, and run with other dedicated kernel namespaces, such as the process and mount namespaces. Once kubernetes schedules a Pod to a cluster node, all containers from the Pod run in the same node. Else, they would not be able to share Kernel namespaces.

image::s3-workloads-lecture-fig-4.svg[]

Some containers from a Pod may run sequentially, for example the init containers, which perform whatever initialization that is required before starting the main application container, and multiple initialization tasks may be required in strict order.

Other containers from a pod may run in parallel, for example the side-car containers, which complement the main application container with support tasks such as network proxying and access control.

It is common to have Pods which run a single container, but the possibility of running multiple containers on the same Pod enables reusing container images as building blocks or larger application components.

It is a recommended practice to avoid joining large application components in the same Pod, for example: running a web application and its database on the same Pod is considered an anti-pattern.

It may seem intuitive to run a web application and its database on the same Pod, especially if that application is used to connect to its database on localhost, but it also impacts Kubernetes ability to manage the performance and high availability of those components, because they must run together, in the same Kubernetes cluster node, and be scaled together.

For that example of an application and its database, it would be better to have the ability to spread the web application and its database on different nodes, and being able to run scale Pods of the web application independently of their database Pods.

=== References to Pods and Their Workload Controllers

It is common to see Pod resource instances named with a random hash as a suffix of their names: this is usually a sign that the Pod instance was created by a workload controller, such as a Replica Set or Job, and that resource controller must generate unique names for each of its Pods.

A resource controller may leave old Pods around, so a Kubernetes Operator may inspect its termination state and configuration settings, or may purge (delete) them as soon as they are not needed anymore. Each resource controller will have different policies for keeping or purging their old pods.

Note that pods from a stateful set do not get a random suffix. They require a consistent identity, and new pods take over the name of the original pods they replace.

Most workload controllers do not refer to Pods directly by their names. They refer to pods by their https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/[labels]. Any Kubernetes resource instance can include multiple labels, and Kubernetes Operators can search for those instances by label.

This way, any workload controller can manage multiple pods, such as replicas in a Replica Set or retries of a Job, without storing their unique names. And, from the labels, you can infer which workload controller instance manages (or owns) a Pod instance.

Other Kubernetes API resource types use labels in a similar way, for example:

* Kubernetes Services, which are network load balancers, refer to Pods by label instead of by IP address or by name.

* Kubernetes Network Policies are network firewalls that control network traffic between Pods in the same or different Namespaces by labels on the Pods and Namespaces instead of by IP address range.

Whenever a Kubernetes API resource instance manages another resource instance, it is expected that it sets an https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/[owner reference] in the managed instance. For example, a Replica Set instance sets itself as the owner of all Pod instances it creates, and a Deployment instance sets itself as the owner of all Replica Sets it creates. 

Well-designed add-on operators will set owner references all the way, so you can track any Pod and other resources to the add-on operator which ultimately manages them.

== Kubernetes Application Configuration Resources

Workload controllers and Pods are not sufficient to describe typical cloud-native and other kinds of applications or components. At the beginning of this section we mentioned other application resources from Kubernetes, such as Services, Configuration Maps, and Persistent Volume Claims. Add-on operators could add more application resource types or add more functionality over standard Kubernetes resource types.

image::s3-workloads-lecture-fig-5.svg[]

The next chapter will present Kubernetes standard API resources and OpenShift custom resources which deal with persistent storage and networking, but here we introduce two resource types which provide applications with their configuration settings:

Configuration Maps::

A configuration map (or `ConfigMap`) provides key/value pairs. Pods refer to those keys to initialize operating system environment variables on containers, or to initialize configuration files which are mounted on their containers.

Secrets::

Secrets are similar to configuration maps, except that their values can be binary data, and they are never stored on disk on compute nodes. These differences make Secrets a bit better suited for sensitive data such as passwords and digital certificates.

Red Hat OpenShift enforces encryption of data from Secrets, as they are on transit from an OpenShift control plane to its compute nodes, and can be configured for encryption at rest, for secret data stored in an OpenShift control plane. Add-on operators, such as the https://www.redhat.com/en/blog/how-to-setup-external-secrets-operator-eso-as-a-service[External Secrets] add-on operator and the https://www.redhat.com/en/blog/introducing-the-secret-store-csi-driver-in-openshift[Secrets Store CSI driver] add-on operator can be enabled to improve the security of secrets at rest and on transit.

Most application workloads, and OpenStack services are not exception, except configuration data stored in Kubernetes configuration maps and secrets resource instances. Some of them are mandatory when deploying Red Hat OpenStack Services on OpenShift, and you will see them in the `openstack` namespace; others are generated at runtime by the OpenStack add-on operators and its child operators, which produce complete and raw configuration files, as expected by OpenStack services.
