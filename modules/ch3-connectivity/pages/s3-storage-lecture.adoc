:time_estimate: 10

= Kubernetes Storage for OpenStack Services

_Estimated reading time: *{time_estimate} minutes*._

Objective::

Describe how OpenStack services use Kubernetes PVCs, storage classes, and direct access to back-end storage for internal needs and to support application workloads.

WARNING: Pending Review

== OpenStack and Kubernetes Storage

Unlike networking, Red Hat OpenStack Services on OpenShift was designed to work with standard Kubernetes storage. In fact, local node storage services available with Red Hat OpenShift should be sufficient for many OpenStack clusters, and any network storage provider, for exemple OpenShift Data Foundation, can also be used.

Red Hat OpenStack Services on OpenShift only uses Kubernetes storage for internal infrastructure services. User-facing services, such as Cinder and Glance, do not use Kubernetes storage. They connect directly to the storage backeds that compute nodes connect to provide storage for applications running as VMs, and for that they require access to the same storage networks that compute nodes do.

Some OpenStack services, such as Glance and Swift, can run entirely on the control plane and use Kubernetes storage, but this is not recommended for production deployments. It is better that those services connect directly to storace backens without using Kubernetes, in the case of Glance, or that storage backends take over the object storage service, in the case of Swift.

== Kubernetes Storage Concepts

Kubernetes Pods can mount volumes with multiple purposes, and individual containers on the same Pod can share volumes but have independent mount namespaces. Volumes can use configuration data from Configuration Maps and Secrets, or ephmeral storage, besides persistent storage.

All Pods use ephemeral storage by default, meaning that data stored on those volumes can be lost after a Pod terminates. Ephemeral storage may be backed by local storage form Kubernetes cluster nodes or by in-memory filesystems. Either way, ephemeral storage is local to a node and not resilient. 

If a Pod uses persistent storage for a volume, it means the data stored on that volume is retained after a Pod terminates, and can be attached to a different Pod, possibly running on a different cluster node. Remember that Kubernetes workload controllers do not restart terminated pods, they create new pods to take their places, and the new pods require access to the same persistent storage volumes as the old pods.

=== Kubernetes API Resources for Persistent Storage

Standard Kubernetes provides three API resource types related to persistent storage:

image::s3-storage-lecture-fig-1.svg[alt="Kubernetes API storage resources & dynamic storage components"]

Persistent Volume Claims (PVCs)::

Represent a volume in application pods. It specifies desired characteristics of the volume, such as size and access modes, and Kubernetes tries to find a volume, or create a new volume, with the closest possible match.

Persistent Volumes (PVs)::

Represent a volume on backend storage. They might be actual devices, such as LUNs from iSCSI and Fibre Storage arrays, or might be file shares in a neworked file system such as NFS and CephFS.

Storage Classes::

Represent a class of storage volumes with similar characteristics and technologies. Multiple classes could represent different tiers of price/performance on the same storage backedn or completely different storage backends

Kubernetes Operators create and manage PVCs together with other application resources, such as Deployments, Services, and Secrets. But Kuberentes Administrators cannot manage PVs and storage classes, unless they are cluster administrators.

=== Shareable Storage and Access Modes

The same PVC can be mounted in multiple Pods but Kubernetes itself does nothing to make shared storage safe. If applications and backend storage do not collaborate to prevent data consistency issues, data stored in shared storage may become corrupt.

Some storage technologies, such as shared file systems, are designed to share storage volumes and files among multiple servers and provide advanced locking and caching features. Others, for example remote block devices, provide no features to help applications ensure data consistency other than locking revices for exclusive access. NFS and CephFS are examples of the first, while iSCSI ad Ceph RBD are examples of the later. But applications can still be designed to share remote block devices.

Each Kubernetes PVCs and PVs define the https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes[acess modes] they support, for example single-node read-write (RWO) or multiple node read-only (ROX). 

IMPORTANT: Most of Kubernetes access modes work at the cluster node level instead of the pod level and do not work as an exclusive access gate. That means a single-node volume (RWO) can be shared by multiple pods running on the same cluster node. Do not rely on Kubernetes access modes alone to prevent data integrity issues.

The features which make sharing storage easier and safer may impact performance and reliability for applications which require high I/O trhoughput or transactional consistency, such as databases and messaging servers. Shareable storage from networked file systems is usually not recommended for such workloads.

=== Manually and Dynamically Provisioned Volumes

Kubernetes Administrators could precreate multiple PVs, each configured to different local or remote storage devices or file shares, and PVCs bind to those manually provisioned PVs on demand, as long as there are still PVs unbound. This is the only alternative for Kubernetes clusters with a dynamic storage provisioner, and standard Kubernetes provides no such component.

Manually provisioned PVs take advantage of standard iSCSI and NFS clients embeded into a Linux kernel, and can take advantage of any other storage client installed on cluster nodes,but they require too much management overhead.

Manually provisioned PVs can be assigned storage classes to group PVs backed by similar remote or local storage. PVCs can them specify the desired storage class and this way a Kubernetes operator has control of the type of remote storage accessed by their applications. If a PVC requests a storage class but there are no unbound volumes from that class, the PVC remains unbouned and any Pod which mounts that PVC as a volume cannot start. Both PVC and Pod remain in a pending state.

Storage classes are more frequently used together with dynamic storage provisioners. If a provisioner detects a pending PVC which requests its storage class, it creates a new PV on demand and binds the PVC to it. The storage provisioner encapsulates the management API of the remote storage, which is most times proprietary to a storage vendor and product. Most standard remote storage protocols, such as iSCSI and NFS, do not define a management API.

Standard Kubernetes provides no storage provisioners. There are a number of open source storage provisioners for standard protocols such as NFS but they are not considered production-grade.

Dynamic storage provisioners did not elliminate all the need for manually provisioned PVs. For example, to shared a remote filesystem with applications running outside a Kubernetes cluster.

=== Constainer Storage Interface (CSI) Drivers with Red Hat OpenShift

Managing storage for production aplications require capabilities absent from the PVC and PV API resource types. Over time, the Kubernetes defined new API resources for those capabilities, such as volume snapshots and copy-on-write volumes (also knows as "thin" volumes) and Kubernetes required a way of knowing if a given storage backend supports those capabilities or not.

The https://kubernetes.io/docs/concepts/storage/volumes/#csi[Container Storage Interface (CSI)] provides both a standard way of deploying storage provisioners and of enmerating their capabilities. Nowadays you expect that all storage vendors provide CSI drivers for their products and that they support advanced capabilities such as snapshots.

Software-defined storage solutions, such as Red Hat Ceph Storage, are exepcted to provide CSI drivers for Kubernetes too. In fact, https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation[Red Hat OpenShift Data Foundation] provides CSI drivers for either remote Ceph clusters or for running Ceph as containers inside an OpenShift cluster, backed by local disks on OpenShift cluster nodes.

Red Hat also provides two add-on operators which provide CSI drivers using local storage from OpenShift cluster nodes:

Local Storage Operator (LSO)::

Supports any directly-attached device on a node, and it usually exposes those devices as PVs to be consumed by a higher-level software-defined storage solution such as OpenShift Data Foundation.

Local Volume Manager Storage Operator (LVMS)::

Enables using the Linux Local Volume Manager (LVM) to dynamically creating new local volumes. It takes over a Volume Group (VG) and creates new Logical Volumes (LVs) for new PVs.

It may be counter-intuitive using local storage to back PVCs and pin Pods to a Kubernetes cluster node: If that cluster node fails, all data on its volumes is lost and those Pods cannot be recreated on surviding cluster nodes.

But a class of applications, such as noSQL databases, are designed to replicate and shard data by themselves, and for them the speed, high throughput, and low latency of local devices may be advanteous compared to remote storage. If those applications are designed to run as multiple pods, each in a different cluster node, they may be able to recreate data from a failed instance by using the surving instances.

== PVCs for OpenStack Internal Services

Red Hat OpenStack Services on OpenShift was designed to only require Kubernetes storage for selected infrastructure service, and those services are designed to handle data resiliency by themselves, in a way that local storage from either the LSO or LVMS are sufficient and supported for production clusters.

image::s3-storage-lecture-fig-2.svg[alt="OpenStack services using Kubernetes storage or embedding storage clients"]

If you prefer using a remote storage solution with a CSI driver certified for Red Hat OpenShift, you can, but you do not require such solution. You're recommended to use the same storage class for all internal services, for simplicity, and also because they share similar workload characteristics of transactional, high IOPS throughput. But you could use different storage classes for each of them.

The following OpenStack services require Kubernetes storage:

MariaDB::

Each database instance requires one PVC for storing OpenStack API resource instances from all OpenStack user-facing services in the cluster or in a cell. It is recommended that you run three MariaDB instances per OpenStack compute cell, managed by a Galera instance, to ensure no data loss in case of an OpenShift node failure. 

RabbitMQ::

Each AMQP messaging server requires one PVC for storing in-flight messages between OpenStack service components and subscriber lists. Similar to MariaDB, it is recommended that you run three RabbitMQ instances per OpenStack compute cell, but RabbitMQ manages data replication and load balancing by iself, without any external component such as Galera.

OVN::

The Open Virtual Network software-defined networking layer requires two network flow databases, the north bound and south bound, which could be recreated at a cost from by Neutron but are stored on disks and replicated between multiple OVN pods and their PVCs, using the RAFT protocol, to ensure smooth performance and resilience of large OpenStack clusters.

Because MariaDB, RabbitMQ, and OVN handle data consistency and resilience by themselves, there is no need for local storage high availability features, such as RAID, on OpenShift cluster nodes running OpenStack services. If you see value on such features and wish to useuse them, just ensure and they are designed to avoid data consistency issues by using writethough caches or writeback caches with bateries.

Notice the recommendation for three instances of MariaDB abd RabbitMQ per cell, and of each OVN database per cluster. It is recommended that clustered systems run an even number of instances to avoid split brain scenarios, where a group of instances cannot connet to other group. You must ensure one group "wins" because it is the larger group. If you need more instances, for scalability purposes, increase from three to five or seven. Or consider running more OpenStack compute cells, each with dedicated MariaDB and RabbitMQ instances, rather than larger database and AMQP clusters.

If you configure a proof-of-concept OpenStack cluster which uses Kubernetes storage for user-facing services such as Glance and Swift, be warned that those services are not tested for either scalability nor resiliency with Kuberbetes storage. They are designed to work directly with backend storage and to scale by themselves without depend on Kubernetes PVCs nor CSI drivers.

// Here learners would ask, but I have no info from the BU on the subject: how to backup and restore data from OpenStack control planes (just MariaDB?) for DR?
