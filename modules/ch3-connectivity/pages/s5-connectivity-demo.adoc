= Demo: OpenStack Networking and Storage

Objective::

Identify Kubernetes storage and networking resources, including custom resources, that provide connectivity for OpenStack services.

WARNING: Work In Progress

== Recording

This demonstration will be recorded and the video linked from here, so learners can watch the video and optionally try to replicate it by themselves.

== Environment

There is no demo environment for now. Presenters must be able to provide their own environment, which should be an already configured and fully funcitonal Red Hat OpenStack on OpenShift cluster. It should be configured in a way that it is ready for an OpenStack Administrator, that is, including an OpenShift user with edit or admin roles in the OpenStack operator and workloads projects on OpenShift. Using an OpenShift cluster administrator user is fine for this demonstration but may not be for other demonstrations of this course.

For this demonstration, an empty OpenStack cluster is fine: it will not deal with OpenStack domains, projects, server flavors, floating ips, tenant networks, etc. That may not be fine for other demonstrations of this course.

The demo environment must also provide a user workstation preconfigured with the OpenShift client and a web browser to access the OpenShift web console.

// Add a link/note to the devpreview3 demo where you have to deploy openstack, so you can show the actual RHOSO 18 UI instead of the UI of a previous release of RHOSP?

== Demonstration


1. If your cluster has any networking different than the sample scenario on this chapter, start by showing a diagram of it and highlighting how it differs. Then show the IP address ranges for each network in an annotated version of the same diagram, or an annotated version of the diagram form the first lecture of this chapter.

2. Log in on OpenShift, using its web console. Ensure you're in the Administrators perspective.

3. Enter the Installed Operators page, show the NMState and MetalLB operators, and their required "empty" cluster-level configuration resources.

4. Enter the OpenStack project and show the nncp and net-attach-def resources on it, and explain how each maps to the networks presented initially.

6. List routes and service resources, and relate to the services in the keystone catalog. Highlight the internal API and public API endpoints and which kind of service each users.

7. Show that not all Kubernetes services are listed on Keystone and show that the ones which aren't are ClusterIP services, while the listed ones are LoadBalancer services. Show that all LB services have external IPs on the internalapi network.

8. Pick a sample pod (ideally an easily recognizable one) and show the Multus annotations on it, and explain why that pod requires those specific secondary networks. You can also show a second pod just to highlight it connects to a different set of secondary networks.

9. Oc exec into the same sample pod and list its network devices and do something to connect to one of the isolated networks. Could be a ping, ncat, or whatever.

10. List all PVCs on the openstack project and their related PVs and and storage classes. It should be consistent, not like the current demo env that uses manually provisioned NFS PVs for Glance and ODF for everything else. For a production setup, you should see tree mariadb and three rabbitmq for the same cell and each with a dedicated PVC.

11. If your OpenStack control plane is running with a non-production setup, for example with single instances of MariaDB or RabbitMQ per cell, do not hide it, and warn that it is not a production-level setup. You can show, in the control plane resource, where you would increase the number of instances of each to three.

12. Show storage configuration in the control plane custom resource for cinder or glance, and show the raw configuration of a storage backend, constrast with the configuration for mariadb and rabbitmq, which just referneces a kubernetes storage class (or inherit the global storage class for the control plane).

13. Oc exec to a glance or cinder pod and use the local client (such as librbd) to conect to the storage backend of compute nodes.

14. SSH into a compute node and use a local client on the node to connect to the same storage backend.

This ends the demonstration.
