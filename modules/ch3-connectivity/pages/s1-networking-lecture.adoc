:time_estimate: 20

= Kubernetes Networking for OpenStack Services

_Estimated reading time: *{time_estimate} minutes*._

Objective::

Describe how OpenStack services use OpenShift routes, Multus secondary networks, and MetalLB to enable connectivity with compute nodes.

WARNING: Work In Progress

== OpenStack With Isolated Networks

OpenStack clusters are usually deployed with multiple isolated networks. Isolated networks address data center performance, reliability, and security concerns. It is common to run OpenStack clusters with large physical servers that contain multiple network interfaces.

The following figure illustrates a generic data center environment, just to provide context and invite you to consider how an OpenStack cluster might fit in such environment.

image::s1-networking-lecture-fig-1.png[]

Isolated networks can either be physical layer 2 networks, which map to independent network interfaces in a server, or virtual layer 2 networks (VLANs) which are trunked into a single network interface. Both could exist in the data center. In any case, each network becomes a network interface device on a Linux server.

It is also possible to provide multiple paths, that is, multiple network interfaces, on the same server, which connect to the same layer 2 network. This enables high availability, in case one of the paths (cables and network gear) fails, and aggregating the capacity of all paths for increased bandwidth. In that case, multiple network interfaces are grouped into a single bonded network device on a Linux server.

Isolated networks are restricted by layer 3 routing and border firewalls and cannot connect to each other. For our purpose, an isolated network could be composed of multiple layer 2 networks that are interconnected in a larger layer 3 network, but that larger network is isolated from other layer 3 networks.

If a server must run software which accesses multiple isolated networks, that server needs network interfaces attached to each of them. And, even if those networks have no routing between each other, it is best that you design those isolated networks in a way that they do not conflict with each other, that is, to use disjoint IP address ranges.

Data centers may partition their networks in different ways and include many isolated networks for different purposes, for example:

* A storage management network, isolated from the storage data network, to manage storage arrays.

* An intranet network, which connects applications to internal users of an organization but not to external users such as customers and partners.

* A server management network, which connects to hardware management boards on physical servers, for remote booting, power control, and installing an operating system.

Data centers might also include multiple isolated networks with similar purposes, for example: multiple storage networks, for different storage backends; or multiple external networks, to connect selected applications to different classes of users.

Whatever your network isolation scenario, Red Hat OpenStack Services on OpenShift provides an OpenStack Administrator with control over which networks attach to which OpenStack services and to which application workloads.

=== Reference OpenStack Networking Scenario

The reference design of OpenStack clusters include isolated networks for OpenStack itself and its supporting infrastructure, such as external storage backends, and possibly multiple isolated networks dedicated to application traffic.

image::s1-networking-lecture-fig-2.png[]

All of those isolated networks must be attached to OpenStack cluster nodes, including the OpenShift cluster nodes which run an OpenStack control plane, and different components in the control and data planes of the OpenStack cluster requires connectivity with different isolated networks. In fact, seggregating traffic from different components, and from OpenStack itself from applications, is a major factor driving an isolated network design for OpenStack.

image::s1-networking-lecture-fig-3.png[]

In the previous example, OpenStack clusters connect to the following networks:

Control plane network (ctlplane)::

Connects an OpenStack control plane to its compute nodes. It is the network that OpenStack uses to manage compute nodes, at the operating system level, to deploy and configure the components of OpenStack services which must run on compute nodes.

Internal API network (internalapi)::

Connects OpenStack components running on compute nodes to other components of the same service and also other OpenStack services running on the control plane. It is the network that OpenStack uses to manage applications.

Tenant or application network (tenant)::

Connects workloads running on compute nodes to each other. It is the network that tunnels traffic from all virtual tenant networks from Neutron.

Storage network (storage)::

Connects OpenStack compute nodes to storage back-ends, to store application data manged by  OpenStack services, such as Cinder and Glance, on behalf of applications.

External network (external)::

Connects applications running on compute nodes to end users and other applications running outside of their OpenStack cluster. It is a provider network on Neutron.

The Red Hat OpenStack team maintains a few https://github.com/openstack-k8s-operators/architecture/tree/main[Validated Architectures] which are variations of this reference architecture. Each Validated Architecture includes sample configurations for the OpenStack add-on operator and they address scenarios such as Hyper-converved Compute Nodes (HCI), which are OpenStack compute servers which are also Red Hat Ceph Storage nodes.

== Kubernetes Networking Basics

Kubernetes was designed for a very simple networking model, at least when compared to OpenStack: all pods attach to a single, flat virtual layer 3 network, which exists only inside a Kubernetes cluster.

image::s1-networking-lecture-fig-4.png[]

If a Pod must connect to something outside its Kubernetes cluster, its cluster node performs SNAT, and that something sees packets as they come from the node.

If something outside a Kubernetes cluster must connect to a Pod, there are a few alternatives:

1. Running Pods on the host network namespace, so those Pods connect directly to the same networks as their hosts.

2. Create a Kubernetes load balancer, represented by a Service API resource, which forwards packets from an external IP to Pods inside the cluster.

3. Create a reverse HTTP proxy, represented by an Ingress API resource, which forwards HTTP requests from an external IP address to Pods inside the cluster.

Option (3) above is the most popular with cloud native applications, because it mixes well with API gateways and other cloud application design patters. It also eases network management, because a singe reverse proxy instance, thus a single external IP address, can be shared with many application instances.

Option (1) is not encouraged because privileged Pods, running on host namespaces, increase the risk of security issues. It is typically reserved for infrastructure components which provide networking services for applications, for example: running reverse HTTP proxies as Pods.

Finally, option (2) is not possible with standard Kubernetes, at least on physical machines.

=== Kubernetes Services

Services are an essential concept of Kubernetes networks. Pods should not directly connect to each other, but connect through Services. 

// Should the next para be in chapter 2?

Pods in Kubernetes are expected to be ephemeral. If a Pod (or a container inside a pod) terminates, for whatever reason, it is not restarted. Instead, its resource controller creates a new Pod. And every new Pod gets a new IP address on the cluster network.

So Pods do not have stable IP addresses which other Pods could use to connect to them. Services provide stable IP addresses and also DNS names which are resolvable by all Pods inside a Kubernetes cluster.

A Kubernetes Service finds its Pods by means of a pod selector which matchs labels on Pods. Usually a Service uses the same labels that a workload controller, such as a Deployment, uses to match those pods. But it doesn't have to use exactly the same labels, which enable sophisticated processes such as A/B application testing and canary deployments.

There are many types of Kubernetes Services, and the main ones are:

ClusterIP::
Are internal load balancers which fronts Pods for traffic originating from other Pods in the same cluster.

LoadBalancer::

Are external load balancers which fronts Pods for traffic originating outside of their clusters.

Only the `CluserIP` service type is guaranteed to work in any Kubernetes cluster. Other types of services depend on infrastructure outside of the cluster.

Kuberentes external load balancers, that is, Services of type `LoadBalancer`, have both an internal IP, inside the virtual internal network, and an external IP, or Virutal IP (VIP), which connects the load balancer to outside of a cluster. 

=== Kubernetes Ingress and OpenShift Routes

OpenShift Route custom resources and Kubernetes Ingress resources serve similar purposes: reverse HTTP proxying, which enables traffic from outside a cluster to reach Pods inside a Kubernetes cluster. Routes and Ingress differ on syntax and minor features, and OpenShift Routes predate Kuberntes Ingress, much makes it more common among OpenShift users.

Routes and Ingress resources are the preferred way of enabling connectivity from end users and external client applications. Applications which use other layer 7 protocols, such as relational databases and AMQP messaging servers, cannot use Routes and Ingress. If they must accept connections from outside their Kubernetes cluster, they must use other approaches, for example external load balancers.

=== Limitations of Kubernetes Networking

Kubernetes was originally designed to run cloud-native applications on cloud provider instances. Over time, the open source community created extensions to make Kubernetes suitable for a wider variety of workloads and on different kinds of IT infrastructure, including traditional hypervisors and physical servers. Red Hat OpenShift comes with many of these extensions already bundled in.

Some of these capabilities require resource controllers or add-on operators which are not included with standard Kubernetes. In particular, Kubernetes does not provide resource controllers for Ingress resources in any environment, nor for external load balancer services outside of cloud providers.

Red Hat OpenShift fills those gaps with the Ingress cluster operator, which provides an an Ingress controller, and the MetalB add-on operator, which supports external load balancer services.

=== Kubernetes with Isolated Networks

Standard Kubernetes provides no features to deal with multiple isolated networks. Kubernetes clusters the primary network for all traffic inside and outside the cluster.

Without add-on operators, the only way an application could connect to isolated networks would be running on their Pods on the host network namespace. This is not a general-purpose solution for application workloads. Fortunately, Red Hat OpenShift comes with support for Multus secondary networks and MetalLB load balancers, which we present later in this section.

== OpenShift Extensions to Kubernetes Networking

During the presentation of standard Kubernetes networking, we already mentioned two OpenShift extensions:

* Route custom resources, which offer an alternative to Kuberntes Ingress resources.

* MetalLB, an OpenShift add-on operator which enables Kubernetes Services to work as external load balancers on physical servers.

While Routes and Ingress do not provide access to multiple isolated networks, MetalLB can be configured with virtual IPs of different networks, over multiple network devices on OpenShift cluster nodes.

The MetalLB add-on operator solves the connectivity needs from OpenStack compute nodes to OpenStack Pods, but doesn't solve the other way around, and it also does not solve the connectivity requirements from OpenStack Pods to multiple isolated networks. For that, Red Hat OpenStack Services on OpenShift uses two other features of OpenShift: Multus and NMState.

[ Figure of Kubernetes pod/service networks, SNAT, services, and ingress + secondary networks and MetalLB ]

* Multus enables OpenShift to attach Pods to any number of secondary networks. Those networks are network devices on OpenShift cluster nodes, which must be preconfigured network interfaces on physical cluster nodes.

* NMState enables configuring physical network interfaces with VLANs, bondings, or whatever kind of hardware and layer 2 connectivity is desired. It also enables configuring a number of other Linux networking features such as virtual bridges, but this is beyond the scope of this course.

=== Multus Secondary Networks and Pods

Multus is a Container Network Interface plugin (CNI) which enables attaching multiple virtual network interfaces to Pods. Multus enables many interesting features, by the use of a plug-in architecture and a JSON configuration syntax, which are beyond the scope of this course.

Network Attachment Definition custom resources represents secondary networks. Once there is a Network Attachment Definition in a project, Pods use annotations to declare the secondary networks they attach to.

[ Figure of kuberentes application resources + Multus and NMState resources ]

Each secondary networks becomes an additional virtual network device inside all containers of a Pod. Any OpenShift Operator can create Pods that attach to any secondary network in the same project.

Traffic to other Pods and to Kubernets Services still flow through the primary network interface from Kubernetes, but secondary networks enable Pods to both start and listen to network connections on those secondary networks, over any network protocol.

=== Network Interface Configuration with NMState

NMState offers a declarative syntax for configuring Linux network interfaces using NetworkManager. The NMState add-on operator manages NMState configurations and applies them to OpenShift cluster nodes.

Before NMState, OpenShift Administrators had to configure host networking with cumbersome approaches, such as using kernel arguments at RHEL CoreOS boot time, or using low-level Machine Configuration resources to feed OpenShift cluster nodes with Network Manager configuration files. With the NMState add-on operator the configuration syntax is simpler and changes do not require a node reboot.

A Node Network Configuration Policy custom resource instance represents the network settings of possibly multiple network interfaces on many OpenShift cluster nodes. You do not need one instance for each cluster node, as long as a group of nodes have similar hardware, with the same device names and are attached to the same physical networks, and get their IP addresses from external means such as a DHCP server.

But, if you need static IP addresses on each OpenShift cluster node, or those nodes have varying hardware configurations, you can create a Node Network Configuration Policy custom resource instance for only one node.

=== The OpenShift Network cluster operator

The OpenShift Network Configuration custom resource (`network.cluster.openshift.io`) provides the IP address ranges of the pod and service networks. It has a single non-namespaced instance named `cluster` and you should query two attributes:

* `spec.clusterNetwork` for the IP ranges for Pods.
* `spec.serviceNetwork` for the IP ranges for Services.

The default ranges, which could be changed at OpenShift installation time, are in the following example:

[source,subs="verbatim,quotes"]
--
$ oc get network cluster -o jsonpath='{.spec.clusterNetwork}{"\n"}'
[{"cidr":"10.128.0.0/14","hostPrefix":23}]
$ oc get network cluster -o jsonpath='{.spec.serviceNetwork}{"\n"}'
["172.30.0.0/16"]
--

There is no easy way of finding the IP ranges of the Kubernetes primary network. You could just check the IP addresses of individual cluster nodes, from the Node resource intances, and guess which is their subnet range, or ask an OpenShift Administator.

// JFYI the web console lists "management address" for nodes, which is for BMC, not for the primary network

From the perspective of Kubernetes, its primary network does not need to be an isolated network. Kubernetes only requires IP connectivity between its cluster nodes over a wide range of TCP ports. There are additional latency consideration between Kubernetes control plane nodes, but they are beyond the scope of this course.

== OpenStack Pods and Kubernetes Networking 

Red Hat OpenStack Services on OpenShift uses a mix of standard Kubernetes networking and OpenShift networking extensions to provide connectivity between OpenStack services and their components. The following sections describe how.

== OpenStack Usage of Service and Route Resources

OpenStack enables connectivity from external clients and compute nodes to OpenStack services running as Pods using Services and Routes:

[ Figure of openstack services with kubernets networking APIs ]

* OpenShift Route resources for public API entry points. All accesses from OpenStack clients outside the cluster, and to the Horizon dashboard, comes through OpenShift routes and are mediated by the OpenShift Ingress controller.

* Kubernetes Service resources, as external load balancers, for private API entry points, which enables communication between Pods of different OpenStack services and between components of those services running on compute nodes and Pods inside an OpenShift cluster.

* Kubernetes Service resources, as internal load balancers, for components of OpenStack services which require no access from outside an OpenShift cluster, and are acessed only by other Pods.

* Finally Kubernetes Service resources, as external load balancers, for AMQP messaging, which enables communication between components of OpenStack services running on compute nodes with components running on the control plane.

The OpenShift Ingress controller can work only with the Kubernetes primary network, it cannot provide private API entry points for OpenStack services over a different network. That's why Red Hat OpenStack Services on OpenShift must use external load balancers for these entry points. If an OpenStack Pod must invoke OpenStack APIs of other services on its OpenStack cluster, it uses the internal IP address of the external load balancer.

In addition to Services and Routes, Pods from Red Hat Services on OpenShift require connectivity to multiple isolated networks to start network connections to storage backends and SSH management of compute nodes, which are not possible with standard Kubernetes networking but are possible with Multus, included with Red Hat OpenShift.

=== OpenStack Pods and Isolated Networks

The OpenShift Network cluster operator enables Multus alongside a primary CNI plugin based on OVN, the same virtual networking layer that Red Hat OpenStack Services on OpenShift uses for Neutron by means of the OpenStack OVN child operator. 

Those two OVN instances cannot mix with each other: one set of OVN pods manage Kubernetes networking, another set of OVN pods manage OpenStack networking. Each requires a dedicated network interface for tunneling traffic between cluster nodes.

Not all OpenShift cluster nodes require connectivity to OpenStack networks: only the OpenShift cluster nodes which run OpenStack pods. Depending on the size of your OpenShift cluster and which other workloads it hosts, besides Red Hat OpenStack Services on OpenShift, it could really mean all OpenShift compute nodes, also called worker nodes.

As you consider your isolated networks design for OpenStack clusters, remember to consider the OpenShift cluster nodes network and also its internal Kubernetes primary network, to prevent IP address conflicts. Such conflicts would affect OpenStack Pods and be very hard to track.

Also consider the isolated network design of you storage backends, which might require additional isolated networks conencted to either OpenShift or OpenStack cluster nodes, or maybe to both. As we will see in the next section, OpenShift and OpenStack clusters do not require connectivity to the same storage backends, but OpenStack Pods do require some connectivity to the same storage backends as OpenStack compute nodes.

=== Isolated Networks and VLANs with OpenShift

An OpenShift cluster running Red Hat OpenStack Services on OpenShift requires at least two physical network interfaces: one for the Kubernetes primary network, and another for the OpenStack control plane network, because OpenShift and OpenStack each run their own set of OVN pods.

[ Figure of openshift networks + openstack isolated networks from the server view]

Other OpenStack isolated networks, except for the OpenStack external, could be VLANs on the same physical interface you use for the control plane network. You cannot use a VLAN for the OpenStack control plane network because it is the network over which the Data Plane operator configures compute nodes for all other networks.

[ Figure of openshift networks + openstack isolated networks from the pods view]

Follows an expanded example scenario of isolated networks with both OpenShift and OpenStack networks:

// As I remember, the name of the primary network, cluster, or node network is not consistent between the network operator and the openshift installer

Kubernetes primary network::

Connects OpenShift cluster nodes to each other and to the outside world. This is the network that tunnels all traffic between Kuberentes Pods, in all namespaces, thanks to the OVN pods from OpenShift.

Kubernetes cluster network::

It is the virtual network, internal to an OpenShift cluster, which connects Pods. It includes two subnets ranges, one for Pods, and another for Services.

OpenStack control plane network::

Connects OpenShift worker nodes to OpenStack compute nodes for SSH connections.

OpenStack internal API network::

Connects OpenStack compute nodes to OpenShift worker nodes for access to internal OpenStack API endpoints and AMQP.

OpenStack tenant or Application network::

Connects workloads running on compute nodes to each other. This is the network that tunnels traffic between OpenStack server instances, thanks to the OVN pods from OpenStack.

OpenStack storage network::

Connects OpenStack compute nodes and OpenStack Pods to the storage back-ends of Cinder and Glance.

OpenStack external network::

Connects workloads running on compute nodes to anything outside their OpenStack cluster.

All OpenStack isolated networks, except for the external network, and including the control plane network, must be configured on OpenShift as Multus secondary networks, with their network interfaces configured using NMState. Beware that these configurations must be consistent with the Network Configuration custom resource from the OpenStack Infrastructure add-on operator.

It may seem strange configuring the OpenStack tenant network as a Multus secondary network, because it carries traffic between OpenStack server instances instead of between OpenStack service components, but there are OpenStack Pods which connect to that network, for example the internal DNSmasq server of the OpenStack cluster.

Only the OpenStack internal API network requires MetalLB Virtual IP addresses for internal API endpoints and RabbitMQ. That network also requires a Multus secondary network, because OpenStack service components running as Pods may connect to components running on an OpenStack compute node.

A common simplification, especially for smaller clusters, is using the Kubernetes primary network as the OpenStack external network. This works because individual Kubernetes cluser nodes require connectivity to the outside world and that traffic is not usually blocked by a firewall.

